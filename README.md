# Cross-Publication Insight Assistant - Enhanced

A sophisticated multi-agent system powered by CrewAI for analyzing research publications across multiple sources, identifying trends, and generating actionable insights. This enhanced version features a modern architecture with comprehensive testing, security, and user interface capabilities.

## âœ… Production-Ready Features

This project includes all essential production features:

- âœ… **Complete, Functional System Code**: Fully implemented multi-agent workflow
- âœ… **Clear Setup & Usage Instructions**: Comprehensive documentation below
- âœ… **Testing Suite**: 61+ tests covering unit, integration, and end-to-end scenarios
- âœ… **Security Guardrails**: Input validation, domain restrictions, content sanitization, rate limiting
- âœ… **Modern UI**: Streamlit-based web interface with real-time analysis
- âœ… **Environment Configuration**: `.env.example` file with all required variables
- âœ… **Logging & Monitoring**: Comprehensive logging system with configurable levels
- âœ… **No Hardcoded Secrets**: All sensitive data managed through environment variables

## ğŸš€ Features

### Core Capabilities
- **Multi-Agent Analysis**: CrewAI-powered agents for publication analysis, trend aggregation, and insight generation
- **Advanced Web Scraping**: Secure scraping with rate limiting, retry mechanisms, and domain validation
- **NLP Processing**: Multi-approach keyword extraction using NLTK, spaCy, and transformers
- **Statistical Analysis**: Comprehensive trend analysis with clustering and diversity calculations
- **Interactive UI**: Streamlit-based web interface with real-time analysis and visualizations

### Enhanced Features
- **Security First**: Input validation, domain restrictions, content sanitization
- **Production Ready**: Comprehensive error handling, logging, and monitoring
- **Scalable Architecture**: Modular design with configurable components
- **Extensive Testing**: Unit, integration, and performance tests with 61+ test cases
- **Rich Visualizations**: Interactive charts and graphs using Plotly
- **Export Capabilities**: Multiple output formats (JSON, CSV, PDF reports)

## ğŸ—ï¸ Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UI Layer      â”‚    â”‚  Agent Layer    â”‚    â”‚  Tool Layer     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Streamlit App â”‚â—„â”€â”€â–ºâ”‚ â€¢ Publication   â”‚â—„â”€â”€â–ºâ”‚ â€¢ Web Scraper   â”‚
â”‚ â€¢ CLI Interface â”‚    â”‚   Analyzer      â”‚    â”‚ â€¢ Keyword       â”‚
â”‚ â€¢ API Endpoints â”‚    â”‚ â€¢ Trend         â”‚    â”‚   Extractor     â”‚
â”‚                 â”‚    â”‚   Aggregator    â”‚    â”‚ â€¢ Data Analyzer â”‚
â”‚                 â”‚    â”‚ â€¢ Insight       â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚   Generator     â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ CrewAI & Flows  â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ Crew          â”‚
                    â”‚   Orchestration â”‚
                    â”‚ â€¢ Flow          â”‚
                    â”‚   Management    â”‚
                    â”‚ â€¢ State         â”‚
                    â”‚   Tracking      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Structure
```
src/
â”œâ”€â”€ agents/              # CrewAI agents for specialized tasks
â”‚   â”œâ”€â”€ publication_analyzer.py    # Content analysis agent
â”‚   â”œâ”€â”€ trend_aggregator.py       # Trend identification agent
â”‚   â””â”€â”€ insight_generator.py      # Strategic insights agent
â”œâ”€â”€ crews/              # CrewAI crew configurations
â”‚   â””â”€â”€ publication_crew.py       # Main crew orchestration
â”œâ”€â”€ flows/              # Advanced workflow management
â”‚   â””â”€â”€ publication_flow.py       # Flow-based processing
â”œâ”€â”€ tools/              # Enhanced analysis tools
â”‚   â”œâ”€â”€ web_scraper.py            # Secure web scraping
â”‚   â”œâ”€â”€ keyword_extractor.py      # NLP-based extraction
â”‚   â””â”€â”€ data_analyzer.py          # Statistical analysis
â”œâ”€â”€ config.py           # Configuration management
â””â”€â”€ main.py            # Application entry point
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.9+
- OpenAI API key
- Git

### Quick Start
```bash
# Clone the repository
git clone https://github.com/AmmarAhmedl200961/cross-publication-insight-assistant-enhanced.git
cd cross-publication-insight-assistant-enhanced

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment
cp .env.example .env
# Edit .env with your configuration (especially OPENAI_API_KEY)

# Run tests to verify installation
python -m pytest tests/ -v

# Launch the application
python src/main.py --help
```

### Development Setup
```bash
# Install development dependencies
pip install -r requirements.txt

# Install pre-commit hooks (optional)
pre-commit install

# Run comprehensive tests
python -m pytest tests/ -v --cov=src --cov-report=html
```

## ğŸ“– Usage

### Command Line Interface
```bash
# Analyze publications using CrewAI crew
python src/main.py analyze --method crew \
  --publications "https://arxiv.org/abs/2301.00001" \
  --include-entities --include-topics \
  --output results.json

# Analyze from configuration file
python src/main.py analyze --config analysis_config.json

# Run with flow orchestration
python src/main.py analyze --method flow \
  --publications publication_list.json \
  --generate-visualizations
```

### Web Interface
```bash
# Launch Streamlit app
streamlit run ui/streamlit_app.py

# Or use the integrated launcher
python src/main.py ui
```

### Python API
```python
from src.main import CrossPublicationInsightAssistant

# Initialize assistant
assistant = CrossPublicationInsightAssistant()

# Analyze publications
publications = [
    {"url": "https://arxiv.org/abs/2301.00001"},
    {"url": "https://arxiv.org/abs/2301.00002"}
]

result = assistant.analyze_publications(
    publications=publications,
    method="crew",
    include_entities=True,
    include_topics=True,
    generate_visualizations=True
)

# Save results
assistant.save_results(result, "analysis_results.json")
```

## ğŸ”§ Configuration

### Environment Variables
Key configuration options in `.env`:

```bash
# Required
OPENAI_API_KEY=your_api_key_here

# Optional (with defaults)
OPENAI_MODEL=gpt-4o-mini
WEB_SCRAPING_RATE_LIMIT=1.0
SECURITY_ALLOWED_DOMAINS=arxiv.org,scholar.google.com
UI_PORT=8501
LOG_LEVEL=INFO
```

See `.env.example` for complete configuration options.

### Advanced Configuration
The system supports extensive configuration through the `config/settings.py` file:

- **LLM Settings**: Model selection, temperature, token limits
- **Security Settings**: Domain validation, content filtering, rate limiting
- **Analysis Settings**: Keyword thresholds, confidence levels, batch sizes
- **UI Settings**: Themes, visualization options, export formats

## ğŸ§ª Testing

### Comprehensive Test Suite
The project maintains extensive test coverage with **61+ test cases**:

```bash
# Run all tests
python -m pytest tests/ -v

# Run with coverage report
python -m pytest tests/ --cov=src --cov-report=html

# Run specific test categories
python -m pytest tests/test_agents.py -v      # Agent tests (4 tests)
python -m pytest tests/test_tools.py -v      # Tool tests (9 tests)
python -m pytest tests/test_integration.py -v # Integration tests (8 tests)
python -m pytest tests/test_comprehensive.py -v # Comprehensive tests (40 tests)
```

### Test Categories
- **Unit Tests**: Individual component testing (agents, tools, utilities)
- **Integration Tests**: End-to-end workflow testing with real data flows
- **Security Tests**: Input validation, XSS protection, injection prevention
- **Performance Tests**: Memory usage, processing time, concurrent stability
- **Reliability Tests**: Error handling, timeout scenarios, malformed data
- **Mock Tests**: External service simulation for consistent testing

### Test Results Summary
```
61 tests collected covering:
- Web scraping security and reliability
- Keyword extraction with special characters
- Data analysis with large datasets  
- Agent workflow orchestration
- Flow-based processing
- Error recovery and resilience
- Memory usage stability
- Configuration variations
```

## ğŸ”’ Security & Guardrails

### Input Validation & Sanitization
- **URL Validation**: Comprehensive URL format and domain checking
- **Content Sanitization**: HTML/script tag removal and content cleaning
- **Size Limits**: Configurable maximum content size (default: 10MB)
- **Domain Restrictions**: Whitelist-based domain access control

### Rate Limiting & Abuse Prevention
- **Request Rate Limiting**: Configurable requests per minute with burst protection
- **Retry Logic**: Exponential backoff for failed requests
- **Timeout Protection**: Configurable timeouts for all external calls
- **Resource Limits**: Memory and processing time constraints

### Security Features Implemented
```python
# Domain validation example
SECURITY_ALLOWED_DOMAINS=arxiv.org,scholar.google.com,researchgate.net

# Content size limits
SECURITY_MAX_CONTENT_SIZE=10485760  # 10MB

# Rate limiting
WEB_SCRAPING_RATE_LIMIT=1.0  # 1 request per second
RATE_LIMIT_REQUESTS_PER_MINUTE=60
```

### Protection Against
- âœ… **SQL Injection**: Parameterized queries and input validation
- âœ… **XSS Attacks**: Content sanitization and HTML parsing
- âœ… **SSRF**: Domain whitelisting and URL validation  
- âœ… **DoS**: Rate limiting and resource constraints
- âœ… **Data Exposure**: No hardcoded secrets, environment-based config

## ï¿½ï¸ User Interface

### Streamlit Web Application
A modern, interactive web interface for publication analysis:

```bash
# Launch Streamlit app
streamlit run ui/streamlit_app.py

# Access at: http://localhost:8501
```

### UI Features
- **ğŸ“Š Interactive Dashboard**: Real-time analysis progress and results
- **ğŸ“ Multiple Input Methods**: Manual URLs, file upload, or demo data
- **âš™ï¸ Configuration Panel**: Advanced analysis options and settings
- **ğŸ“ˆ Live Visualizations**: Dynamic charts and graphs using Plotly
- **ğŸ’¾ Export Options**: Download results in JSON, CSV, or summary formats
- **ğŸ”„ Real-time Updates**: Progress bars and status indicators

### UI Components
- **Analysis Tab**: Input methods and execution controls
- **Results Tab**: Comprehensive analysis results display
- **Export Tab**: Download and sharing capabilities
- **Configuration Sidebar**: Method selection and advanced options

### Supported Workflows
- **Crew Method**: Sequential agent collaboration
- **Flow Method**: Advanced orchestration with parallel processing
- **Batch Processing**: Multiple publications simultaneously
- **Real-time Monitoring**: Live progress tracking and error reporting

## ğŸ“„ Environment Configuration

### Required Environment Variables
Copy `.env.example` to `.env` and configure:

```bash
# Copy the example file
cp .env.example .env

# Edit with your settings
nano .env
```

### Key Configuration Options
```bash
# --- Required ---
OPENAI_API_KEY=your_openai_api_key_here

# --- Core Settings ---
OPENAI_MODEL=gpt-4o-mini                    # LLM model selection
CREWAI_VERBOSE=true                         # Agent workflow verbosity
WEB_SCRAPING_RATE_LIMIT=1.0                # Requests per second
SECURITY_ALLOWED_DOMAINS=arxiv.org,scholar.google.com  # Allowed domains

# --- Optional Advanced Settings ---
LOG_LEVEL=INFO                              # Logging verbosity
CACHE_ENABLED=true                          # Enable request caching
UI_PORT=8501                                # Streamlit port
ANALYSIS_BATCH_SIZE=10                      # Batch processing size
```

### Security Configuration
```bash
# Domain restrictions (comma-separated)
SECURITY_ALLOWED_DOMAINS=arxiv.org,scholar.google.com,researchgate.net

# Content size limits
SECURITY_MAX_CONTENT_SIZE=10485760          # 10MB limit

# SSL validation
SECURITY_VALIDATE_SSL=true                  # Verify SSL certificates
```

### No Hardcoded Secrets
âœ… All sensitive information is managed through environment variables:
- API keys loaded from `.env` file
- Configuration validation prevents hardcoded secrets
- Test mode supports mock credentials
- Graceful fallbacks for missing optional settings

## ğŸ“Š Logging & Monitoring

### Comprehensive Logging System
Built-in logging and monitoring capabilities:

```bash
# Log files location
logs/
â”œâ”€â”€ app.log              # Main application logs
â”œâ”€â”€ error.log            # Error-specific logs
â””â”€â”€ performance.log      # Performance metrics
```

### Logging Features
- **ğŸ“ Structured Logging**: JSON-formatted logs with metadata
- **ğŸ”„ Log Rotation**: Automatic file rotation (10MB max, 5 backups)
- **ğŸ“Š Multiple Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL
- **â±ï¸ Performance Tracking**: Processing times and resource usage
- **ğŸ” Request Tracing**: Complete request/response logging

### Configuration Options
```bash
# Logging configuration
LOG_LEVEL=INFO                              # Minimum log level
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s
LOG_FILE=logs/app.log                       # Log file location
LOG_MAX_SIZE=10MB                           # Max file size before rotation
LOG_BACKUP_COUNT=5                          # Number of backup files
```

### Monitoring Capabilities
- **ğŸ“ˆ Performance Metrics**: Processing time, memory usage, request counts
- **âš ï¸ Error Tracking**: Detailed error logs with stack traces
- **ğŸ”„ Health Checks**: System status and component availability
- **ğŸ“Š Usage Analytics**: Request patterns and user behavior (anonymized)

### Example Log Output
```
2025-09-03 19:30:40,854 - src.main - INFO - Initializing Cross-Publication Insight Assistant...
2025-09-03 19:30:41,166 - src.crews.publication_crew - INFO - Publication Insight Crew initialized successfully
2025-09-03 19:30:41,377 - src.main - INFO - All components initialized successfully
```

## ğŸ“Š Performance

### Benchmarks
- **Publication Analysis**: ~2-5 seconds per publication
- **Trend Analysis**: ~1-3 seconds for 10 publications
- **Memory Usage**: <100MB baseline, scales linearly
- **Throughput**: 10-50 publications/minute (depending on content)

### Optimization Features
- Async processing where possible
- Intelligent caching
- Batch processing
- Resource pool management

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Process
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass (`pytest tests/`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Code Standards
- Follow PEP 8 style guidelines
- Add docstrings to all public functions
- Maintain test coverage above 90%
- Use type hints where appropriate

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹â€â™‚ï¸ Support

### Documentation
- [Technical Documentation](docs/TECHNICAL.md)
- [API Reference](docs/API.md)
- [Deployment Guide](docs/DEPLOYMENT.md)

### Getting Help
- ğŸ“§ Email: support@example.com
<<<<<<< HEAD
- ğŸ’¬ Discord: <a href="https://discord.com/users/515117034862804992">marto90123</a>
=======
- ğŸ’¬ Discord: [marto90123](https://discord.com/users/515117034862804992)
>>>>>>> 89c15db (Update README with comprehensive feature documentation)
- ğŸ› Issues: [GitHub Issues](https://github.com/AmmarAhmedl200961/cross-publication-insight-assistant-enhanced/issues)

### FAQ

**Q: What's the difference between crew and flow methods?**
A: Crew method uses sequential agent collaboration, while flow method provides more complex workflow orchestration with parallel processing and state management.

**Q: How do I add custom analysis tools?**
A: Extend the base tool classes in `src/tools/` and register them with your agents or crews.

**Q: Can I use other LLM providers besides OpenAI?**
A: Yes, the system supports any LangChain-compatible LLM provider. Update the configuration accordingly.

**Q: How do I handle large-scale analysis?**
A: Use batch processing, enable caching, and consider the flow method for parallel processing of large publication sets.

## ğŸš€ Roadmap

### Upcoming Features
- [ ] Multi-language publication support
- [ ] Advanced citation analysis
- [ ] Real-time collaboration features
- [ ] Enhanced visualization options
- [ ] Mobile-responsive UI
- [ ] API rate limiting dashboard
- [ ] Custom agent training capabilities

### Long-term Goals
- [ ] Federated learning for collaborative insights
- [ ] Integration with academic databases
- [ ] Automated report generation
- [ ] Machine learning model optimization
- [ ] Enterprise deployment options

---

**Built with â¤ï¸ using CrewAI, LangChain, and Streamlit**
